{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76dbc1be",
   "metadata": {},
   "source": [
    "# Evaluation: Syntax\n",
    "\n",
    "Part I of the computational evaluation of AI-generated linked data for [Linking Anthropology's Data and Archives (LADA)](https://ischool.umd.edu/projects/building-a-sustainable-future-for-anthropologys-archives-researching-primary-source-data-lifecycles-infrastructures-and-reuse/), focused on syntax (e.g., do the metadata adhere to the expected serialization formats?).\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "I. [Data Loading](#data-loading)\n",
    "\n",
    "II. [Syntax](#syntax)\n",
    "\n",
    "  * [XML](#xml)\n",
    "  \n",
    "  * [JSON](#json)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d2e7c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import config\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c654d",
   "metadata": {},
   "source": [
    "Create variables to reference existing directories and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_path = \"cleaned/dublin_core/\"  # XML data files\n",
    "schema_path = \"cleaned/schema_org/\"   # JSON data files\n",
    "cidoc_path = \"cleaned/cidoc_crm/\"     # JSON data files\n",
    "\n",
    "dublin_t1_dir = config.task1_data+dublin_path\n",
    "schema_t1_dir = config.task1_data+schema_path\n",
    "cidoc_t1_dir = config.task1_data+cidoc_path\n",
    "\n",
    "dublin_p1_dir = config.playgrd1_data+dublin_path\n",
    "schema_p1_dir = config.playgrd1_data+schema_path\n",
    "cidoc_p1_dir = config.playgrd1_data+cidoc_path\n",
    "\n",
    "dublin_p3_dir = config.playgrd3_data+dublin_path\n",
    "schema_p3_dir = config.playgrd3_data+schema_path\n",
    "cidoc_p3_dir = config.playgrd3_data+cidoc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842c841",
   "metadata": {},
   "source": [
    "Create a folder to store the error reports in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"syntax\"\n",
    "report_dir = f\"data/error_reports/{d}/\"\n",
    "Path(report_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185155a0",
   "metadata": {},
   "source": [
    "## Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280c775",
   "metadata": {},
   "source": [
    "### XML\n",
    "\n",
    "First, read and evaluate only the files with a `.xml` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".xml\"\n",
    "dublin_file_paths = []\n",
    "dublin_files_t1 = [f for f in os.listdir(dublin_t1_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_t1_dir+f for f in dublin_files_t1]\n",
    "dublin_files_p1 = [f for f in os.listdir(dublin_p1_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_p1_dir+f for f in dublin_files_p1]\n",
    "dublin_files_p3 = [f for f in os.listdir(dublin_p3_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_p3_dir+f for f in dublin_files_p3]\n",
    "dublin_file_paths.sort()\n",
    "total_dcxml_files = len(dublin_file_paths)\n",
    "print(f\"Total Dublin Core {extension[1:].upper()} files:\", total_dcxml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_errors, errored_files = [], []\n",
    "for file_path in dublin_file_paths:\n",
    "    try:\n",
    "        tree = etree.parse(file_path)\n",
    "    except Exception as e:\n",
    "        f_error = {\"file\": file_path, \"exception_type\": type(e), \"exception_message\": str(e)}\n",
    "        syntax_errors += [f_error]\n",
    "        errored_files += [file_path]\n",
    "print(\"Files with errors:\", \n",
    "      len(errored_files), \"of\", total_dcxml_files,\n",
    "      f\"({(len(errored_files)/total_dcxml_files)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a601f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se = pd.DataFrame.from_dict(syntax_errors)\n",
    "pattern = \"^[\\D]+,\"\n",
    "new_exception_col = df_se[\"exception_message\"].apply(lambda x: re.findall(pattern, x)[0][:-1])\n",
    "df_se.insert(len(df_se.columns)-1, \"exception_subtype\", new_exception_col)\n",
    "df_se.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e163e5",
   "metadata": {},
   "source": [
    "Next, evaluate every record by reading the TXT files to:\n",
    "- check whether relevnt DC namespace(s) are present\n",
    "- check whether RDF namespace is present\n",
    "- check whether a prolog is present\n",
    "- check whether a prolog with UTF-8 encoding is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".txt\"\n",
    "dublin_file_paths = []\n",
    "dublin_files_t1 = [f for f in os.listdir(dublin_t1_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_t1_dir+f for f in dublin_files_t1]\n",
    "dublin_files_p1 = [f for f in os.listdir(dublin_p1_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_p1_dir+f for f in dublin_files_p1]\n",
    "dublin_files_p3 = [f for f in os.listdir(dublin_p3_dir) if f.endswith(extension)]\n",
    "dublin_file_paths += [dublin_p3_dir+f for f in dublin_files_p3]\n",
    "dublin_file_paths.sort()\n",
    "total_dctxt_files = len(dublin_file_paths)\n",
    "print(f\"Total Dublin Core {extension[1:].upper()} files:\", total_dctxt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_syntax_errors, more_errored_files = [], []\n",
    "for file_path in dublin_file_paths:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        f_string = f.read()\n",
    "        file_path = file_path.replace(\".txt\", \".xml\")\n",
    "        if not utils.hasDCNamespaces(f_string):\n",
    "            f_error = {\"file\": file_path, \"exception_type\": \"Custom syntax check\", \"exception_subtype\": \"Missing namespace\", \"exception_message\": \"Missing Dublin Core namespace(s)\"}\n",
    "            custom_syntax_errors += [f_error]\n",
    "            more_errored_files += [file_path]\n",
    "        if not utils.hasRDFNamespace(f_string):\n",
    "            f_error = {\"file\": file_path, \"exception_type\": \"Custom syntax check\", \"exception_subtype\": \"Missing namespace\", \"exception_message\": \"Missing RDF namespace\"}\n",
    "            custom_syntax_errors += [f_error]\n",
    "            more_errored_files += [file_path]\n",
    "        if not utils.hasProlog(f_string):\n",
    "            f_error = {\"file\": file_path, \"exception_type\": \"Custom syntax check\", \"exception_subtype\": \"Missing prolog\", \"exception_message\": \"Missing prolog\"}\n",
    "            custom_syntax_errors += [f_error]\n",
    "            more_errored_files += [file_path]\n",
    "        if not utils.hasPrologWithEncoding(f_string):\n",
    "            f_error = {\"file\": file_path, \"exception_type\": \"Custom syntax check\", \"exception_subtype\": \"Missing prolog\", \"exception_message\": \"Missing prolog with UTF-8 encoding\"}\n",
    "            custom_syntax_errors += [f_error]\n",
    "            more_errored_files += [file_path]\n",
    "print(len(custom_syntax_errors), \"additional syntax errors found across\", len(set(more_errored_files)), \"out of\", total_dctxt_files, \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se = pd.concat([df_se, pd.DataFrame.from_dict(custom_syntax_errors)])\n",
    "new_file_col = df_se[\"file\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df_se = df_se.rename(columns={\"file\":\"file_path\"})\n",
    "df_se.insert(1, \"file_name\", new_file_col)\n",
    "df_se.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se.exception_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fae233",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_message_report = pd.DataFrame(df_se.loc[df_se.exception_type == \"Custom syntax check\"].exception_message.value_counts())\n",
    "custom_df = df_se.loc[df_se.exception_type == \"Custom syntax check\"]\n",
    "custom_df = custom_df[[\"exception_type\", \"exception_subtype\", \"exception_message\"]].drop_duplicates()\n",
    "custom_df = custom_df.set_index(\"exception_message\").join(custom_message_report).reset_index()\n",
    "custom_report = custom_df[[\"exception_type\", \"exception_subtype\", \"exception_message\", \"count\"]]\n",
    "custom_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50385223",
   "metadata": {},
   "source": [
    "The `count` column refers to the total occurrence of each exception, so the sum of that column may exceed the total number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a404de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_standard = \"dublin_core\"\n",
    "data_serialization = \"xml\"\n",
    "report_type = \"syntax_error_stats_custom_subtypes\"\n",
    "custom_report.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_syntax_subtypes = pd.DataFrame(df_se.loc[df_se.exception_type != \"Custom syntax check\"].exception_subtype.value_counts())\n",
    "syntax_df = df_se.loc[df_se.exception_type != \"Custom syntax check\"]\n",
    "syntax_df = syntax_df[[\"exception_type\", \"exception_subtype\"]].drop_duplicates()\n",
    "syntax_df = syntax_df.set_index(\"exception_subtype\").join(xml_syntax_subtypes).reset_index()\n",
    "xml_syntax_error_report = syntax_df[[\"exception_type\", \"exception_subtype\", \"count\"]]\n",
    "xml_syntax_error_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_standard = \"dublin_core\"\n",
    "data_serialization = \"xml\"\n",
    "report_type = \"syntax_error_stats_subtypes\"\n",
    "xml_syntax_error_report.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se_subtypes = df_se[[\"file_path\", \"file_name\", \"exception_type\", \"exception_subtype\"]].drop_duplicates()\n",
    "df_se_subtypes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59282269",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_report = pd.DataFrame(df_se_subtypes[[\"exception_type\", \"exception_subtype\"]].value_counts())\n",
    "subtype_report = subtype_report.rename(columns={\"count\":\"file_count\"})\n",
    "subtype_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957243eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errored_files = set(errored_files + more_errored_files)\n",
    "totals_report = pd.DataFrame({\n",
    "    \"exception_type\": [\"TOTAL FILES\", \"FILES WITH EXCEPTION\"],\n",
    "    \"exception_subtype\": [\"NA\", \"NA\"],\n",
    "    \"file_count\": [total_dcxml_files, len(all_errored_files)]\n",
    "    })\n",
    "totals_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8705032",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_errored_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_report = pd.concat([subtype_report.reset_index(), totals_report])\n",
    "proportions = (xml_report[[\"file_count\"]]/total_dcxml_files).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "xml_report.insert(len(xml_report.columns), \"proportion_of_all_files\", percentages)\n",
    "xml_report = xml_report.reset_index().drop(columns=[\"index\"])\n",
    "xml_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3556315",
   "metadata": {},
   "source": [
    "Save the reports as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_standard = \"dublin_core\"\n",
    "data_serialization = \"xml\"\n",
    "report_type = \"syntax_error_stats\"\n",
    "xml_report.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_standard = \"dublin_core\"\n",
    "data_serialization = \"xml\"\n",
    "report_type = \"syntax_errors\"\n",
    "df_se.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ff4b9",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcf079",
   "metadata": {},
   "source": [
    "First, read and evaluate only the files with a `.json` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9579af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".json\"\n",
    "cidoc_file_paths = []\n",
    "cidoc_files_t1 = [f for f in os.listdir(cidoc_t1_dir) if f.endswith(extension)]\n",
    "cidoc_file_paths += [cidoc_t1_dir+f for f in cidoc_files_t1]\n",
    "cidoc_files_p1 = [f for f in os.listdir(cidoc_p1_dir) if f.endswith(extension)]\n",
    "cidoc_file_paths += [cidoc_p1_dir+f for f in cidoc_files_p1]\n",
    "cidoc_files_p3 = [f for f in os.listdir(cidoc_p3_dir) if f.endswith(extension)]\n",
    "cidoc_file_paths += [cidoc_p3_dir+f for f in cidoc_files_p3]\n",
    "cidoc_file_paths.sort()\n",
    "print(\"Total CIDOC-CRM JSON files:\", len(cidoc_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14058",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidoc_file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da00199",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".json\"\n",
    "schema_file_paths = []\n",
    "schema_files_t1 = os.listdir(schema_t1_dir)\n",
    "schema_file_paths += [schema_t1_dir+f for f in schema_files_t1 if f.endswith(extension)]\n",
    "schema_files_p1 = os.listdir(schema_p1_dir)\n",
    "schema_file_paths += [schema_p1_dir+f for f in schema_files_p1 if f.endswith(extension)]\n",
    "schema_files_p3 = os.listdir(schema_p3_dir)\n",
    "schema_file_paths += [schema_p3_dir+f for f in schema_files_p3 if f.endswith(extension)]\n",
    "schema_file_paths.sort()\n",
    "print(\"Total Schema.org JSON files:\", len(schema_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7122e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_paths = cidoc_file_paths + schema_file_paths\n",
    "total_json_files = len(json_file_paths)\n",
    "print(len(json_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_errors = []\n",
    "for json_f in json_file_paths:\n",
    "    with open(json_f) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            f_error = {\"file\": json_f, \"exception_type\": type(e), \"exception_message\": str(e)}\n",
    "            syntax_errors += [f_error]\n",
    "        f.close()\n",
    "print(\n",
    "    \"Files with errors:\", \n",
    "    len(syntax_errors), \"of\", len(json_file_paths),\n",
    "    f\"({(len(syntax_errors)/len(json_file_paths))*100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se = pd.DataFrame.from_dict(syntax_errors)\n",
    "new_file_col = df_se[\"file\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df_se = df_se.rename(columns={\"file\":\"file_path\"})\n",
    "df_se.insert(1, \"file_name\", new_file_col)\n",
    "df_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55495fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se.exception_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3aedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se.exception_message.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557380d",
   "metadata": {},
   "source": [
    "Looking at the files that triggered the above error messages, it seems that often what's missing is the final curly brace.  Also, in one file, the quotes surrounding strings were doubled (e.g., `\"\"@context\"\":...` instead of `\"@context\"`) and in another file, an attempt at a comment was made using `//`.  While the error messages are useful in locating the general source of the error within a file, they're less informative for distinguishing what needs to be changed to correct the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898c0f",
   "metadata": {},
   "source": [
    "Export a reports about the JSON errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf_se = pd.DataFrame(df_se[[\"file_path\", \"file_name\", \"exception_type\"]].drop_duplicates())\n",
    "subdf_se.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_report = pd.DataFrame(df_se[[\"exception_type\"]].value_counts())\n",
    "type_report = type_report.rename(columns={\"count\":\"file_count\"})\n",
    "type_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_report = pd.DataFrame({\n",
    "    \"exception_type\": [\"TOTAL FILES\", \"FILES WITH EXCEPTION\"],\n",
    "    \"file_count\": [len(json_file_paths), len(syntax_errors)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897aba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_report = pd.concat([type_report.reset_index(), totals_report])\n",
    "proportions = (json_report[[\"file_count\"]]/(len(json_file_paths))).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "json_report.insert(len(json_report.columns), \"proportion_of_all_files\", percentages)\n",
    "json_report = json_report.reset_index().drop(columns=[\"index\"])\n",
    "json_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee8031",
   "metadata": {},
   "source": [
    "Calculate how many files are meant to adhere to Schema.org and CIDOC-CRM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdo_error_df = subdf_se.loc[subdf_se.file_name.str.contains(\"sdo\")]\n",
    "cidoc_error_df = subdf_se.loc[subdf_se.file_name.str.contains(\"cidoccrm\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f48b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_types = df_se.exception_type.unique()\n",
    "assert len(exception_types) == 1, \"There are multiple exception types in the JSON files: {}\".format(exception_types)\n",
    "df_type = df_se.loc[df_se.exception_type == exception_types[0]]\n",
    "sdo_type_df = df_type.loc[df_type.file_name.str.contains(\"sdo\")]\n",
    "cidoc_type_df = df_type.loc[df_type.file_name.str.contains(\"cidoccrm\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_report.insert(len(json_report.columns), \"schema_org_files\", [sdo_type_df.shape[0], len(schema_file_paths), sdo_error_df.shape[0]])\n",
    "proportions = (json_report[[\"schema_org_files\"]]/(len(schema_file_paths))).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "json_report.insert(len(json_report.columns), \"proportion_of_schema_files\", percentages)\n",
    "proportions = (json_report[[\"schema_org_files\"]]/(len(json_file_paths))).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "json_report.insert(len(json_report.columns), \"schema_proportion_of_all_files\", percentages)\n",
    "\n",
    "json_report.insert(len(json_report.columns), \"cidoc_crm_files\", [cidoc_type_df.shape[0], len(cidoc_file_paths), cidoc_error_df.shape[0]])\n",
    "proportions = (json_report[[\"cidoc_crm_files\"]]/(len(cidoc_file_paths))).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "json_report.insert(len(json_report.columns), \"proportion_of_cidoc_files\", percentages)\n",
    "proportions = (json_report[[\"cidoc_crm_files\"]]/(len(json_file_paths))).values\n",
    "percentages = [f\"{proportion[0]*100:.2f}%\" for proportion in proportions]\n",
    "json_report.insert(len(json_report.columns), \"cidoc_proportion_of_all_files\", percentages)\n",
    "\n",
    "json_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d786ff",
   "metadata": {},
   "source": [
    "Save the reports as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_standard = \"cidoc-and-sdo\"\n",
    "data_serialization = \"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_type = \"syntax_error_stats\"\n",
    "json_report.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_type = \"syntax_errors\"\n",
    "df_se.to_csv(\n",
    "    report_dir+\"{metadata_standard}_{data_serialization}_{report_type}.csv\".format(\n",
    "        metadata_standard=metadata_standard,\n",
    "        data_serialization=data_serialization,\n",
    "        report_type=report_type\n",
    "        ), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4a8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
